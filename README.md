# COBOL-Explain-Replication

This repository reproduces and extends the work from **["Explaining COBOL Programs with Large Language Models" (ASE 2025)](https://arxiv.org/pdf/2507.02182)**.  
It provides a clean, modular, and modern setup to parse COBOL projects, run explanation pipelines with different LLMs, and evaluate outputs automatically or manually.

Everything is orchestrated via `main.py` for reproducibility.

---

## ğŸ“‚ Project Structure
```

cobol-to-explanation/
â”‚
â”œâ”€â”€ ASE2025                    # Reproducing this work
â”‚
â”œâ”€â”€ src/                       # Core modular Python code
â”‚   â”œâ”€â”€ extractors/            # Artifact extraction modules
â”‚   â”‚   â”œâ”€â”€ cobol_parser.py    # Parses COBOL programs into functions/files/projects (regex/NetworkX/tree-sitter)
â”‚   â”‚   â””â”€â”€ copybook_resolver.py # Resolves COPY statements by inlining external copybooks
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                # LLM agent wrappers
â”‚   â”‚   â”œâ”€â”€ code_agent.py      # Code Processing Agent (Granite / LLaMA variant)
â”‚   â”‚   â”œâ”€â”€ text_agent.py      # Text Processing Agent (GPT-4o-mini / Claude)
â”‚   â”‚   â””â”€â”€ judge_agent.py     # LLM-as-Judge for evaluation alignment
â”‚   â”‚
â”‚   â”œâ”€â”€ pipelines/             # Level-specific workflows
â”‚   â”‚   â”œâ”€â”€ function_pipeline.py # Function-level explanation pipeline
â”‚   â”‚   â”œâ”€â”€ file_pipeline.py     # File-level pipeline (handles short/long files)
â”‚   â”‚   â””â”€â”€ project_pipeline.py  # Project-level merging pipeline
â”‚   â”‚
â”‚   â””â”€â”€ utils/                 # Helper utilities
â”‚       â”œâ”€â”€ config.py          # Lightweight config manager
â”‚       â”œâ”€â”€ metrics.py         # Automatic evaluation metrics (METEOR, chrF, SBERT)
â”‚       â”œâ”€â”€ token_utils.py     # Token counting and short/long classification
â”‚       â””â”€â”€ io.py              # File read/write helpers (CSV, JSON, graphs)
â”‚
â”œâ”€â”€ script/                    # Sequential notebooks
â”‚   â””â”€â”€ extract_artifact.ipynb # Extraction notebook outputting artifacts to data/processed/
â”‚
â”œâ”€â”€ data/                       # Data storage
â”‚   â”œâ”€â”€ projects/              # Raw COBOL projects (from paper's 14 benchmarks or custom)
â”‚   â”œâ”€â”€ processed/             # Extracted artifacts (functions, files, graphs in CSV/JSON)
â”‚   â””â”€â”€ references/            # Benchmark CSVs (function/file/project ground-truth)
â”‚
â”œâ”€â”€ outputs/                    # Generated explanations
â”‚   â””â”€â”€ run_metadata.json      # Lightweight provenance info (model, prompt version, seed)
â”‚
â”œâ”€â”€ evaluations/                # Evaluation tools and results
â”‚   â”œâ”€â”€ manual_evaluation/     # Human-annotated CSVs for evaluation
â”‚   â”œâ”€â”€ llm_judge/             # Scripts and outputs for LLM-as-Judge
â”‚   â”œâ”€â”€ eval_notebook.ipynb    # Notebook to compute metrics & visual checks
â”‚   â””â”€â”€ demo_pipeline.ipynb    # Example end-to-end pipeline run
â”‚
â”œâ”€â”€ samples/                    # Tiny COBOL programs for smoke tests
â”‚
â”œâ”€â”€ tests/                      # Lightweight tests
â”‚   â””â”€â”€ test_smoke.py           # Verify extraction and pipelines on samples
â”‚
â”œâ”€â”€ main.py                     # CLI entry point orchestrating extract â†’ pipeline â†’ evaluation
â”œâ”€â”€ requirements.txt            # Python dependencies (LangChain, LangGraph, networkx, sentence-transformers, etc.)
â””â”€â”€ README.md                   # This file: setup, structure, replication guide
```


### `src/` â€“ Core Source Code
- **`extractors/`**
  - `cobol_parser.py` â†’ Parses COBOL code into functions, files, and project-level graphs. Can use regex + `networkx`, or tree-sitter if installed.
  - `copybook_resolver.py` â†’ Expands `COPY` statements by inlining external copybooks. Ensures completeness of function/file graphs.

- **`agents/`**
  - `code_agent.py` â†’ Agent interface for code-focused LLMs (e.g., Granite, LLaMA). Handles tokenization, chunking, and prompt formatting.
  - `text_agent.py` â†’ Agent interface for general-purpose text LLMs (e.g., GPT-4o-mini, Claude). Useful for higher-level explanations.
  - `judge_agent.py` â†’ LLM-as-a-Judge implementation. Evaluates generated explanations for correctness and coherence.

- **`pipelines/`**
  - `function_pipeline.py` â†’ End-to-end flow for generating function-level summaries.
  - `file_pipeline.py` â†’ Handles file-level summaries, including â€œshort vs longâ€ token-based branching.
  - `project_pipeline.py` â†’ Combines multiple file-level summaries into project-level explanations.

- **`utils/`**
  - `config.py` â†’ Lightweight configuration manager (dicts/env vars, no YAML bloat).
  - `metrics.py` â†’ Automatic evaluation metrics (METEOR, chrF, Sentence-BERT similarity).
  - `token_utils.py` â†’ Helpers for token counting and classifying artifacts as short/long.
  - `io.py` â†’ Read/write helpers for CSV, JSON, and graphs.

---

### `script/`
- **`extract_artifact.ipynb`** â†’ Jupyter notebook for parsing COBOL projects into structured artifacts. Outputs CSV/JSON into `data/processed/`.  
  Mirrors the original repoâ€™s extraction workflow.

---

### `data/`
- **`projects/`** â†’ Raw COBOL projects (the 14 benchmark repos from the paper, or your own).  
- **`processed/`** â†’ Extracted artifacts (functions, files, dependency graphs).  
- **`references/`** â†’ Ground-truth benchmark CSVs for evaluation (function/file/project summaries from the original dataset).

---

### `outputs/`
- Stores generated explanations from pipelines.  
- Each run may also create a `run_metadata.json` with minimal provenance (model name, date, prompt version, seed).

---

### `evaluations/`
- **`manual_evaluation/`** â†’ Human-annotated evaluation CSVs (like in the paper).  
- **`llm_judge/`** â†’ Scripts and results for LLM-as-a-Judge evaluations.  
- **`eval_notebook.ipynb`** â†’ Notebook to run metrics, compare with references, and visualize results.  
- **`demo_pipeline.ipynb`** â†’ (Optional) Example of running an end-to-end pipeline for demo/testing.

---

### `samples/`
- Tiny COBOL program(s) for quick smoke tests of the extraction + pipeline stages.

---

### `tests/`
- **`test_smoke.py`** â†’ Minimal tests to verify environment and pipelines run on `samples/`.

---

### Root Files
- **`main.py`** â†’ CLI entry point. Orchestrates:
  1. Extract COBOL artifacts
  2. Run selected pipeline (function/file/project)
  3. Save outputs and evaluations
- **`requirements.txt`** â†’ Dependencies (LangChain, LangGraph, networkx, sentence-transformers, etc.).
- **`README.md`** â†’ Youâ€™re reading it.

---

## ğŸš€ Submodules:
git submodule add https://github.com/anonymous-987654321/ASE2025.git **ASE2025**